use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// The `ScrapedContent` struct represents the content scraped from a URL.
/// It includes the URL, the content, metadata, and a timestamp.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScrapedContent {
    /// The URL from which the content was scraped.
    pub url: String,
    /// The main content extracted from the URL.
    pub content: String,
    /// A map of metadata key-value pairs extracted from the URL.
    pub metadata: HashMap<String, String>,
    /// The timestamp when the content was scraped.
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

/// The `SearchResult` struct represents the result of a search operation.
/// It includes the search query, the scraped contents, an optional summary, and the processing time.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    /// The search query used to perform the search.
    pub query: String,
    /// A vector of `ScrapedContent` representing the contents found during the search.
    pub contents: Vec<ScrapedContent>,
    /// An optional summary of the search results.
    pub summary: Option<String>,
    /// The duration of time taken to process the search.
    pub processing_time: std::time::Duration,
}

/// The `LLMRequest` struct represents a request to a Language Model (LLM).
/// It includes the model name, the prompt, the temperature, and the maximum number of tokens.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMRequest {
    /// The name of the model to be used for the request.
    pub model: String,
    /// The prompt to be sent to the model.
    pub prompt: String,
    /// The temperature setting for the model, controlling the randomness of the output.
    pub temperature: f32,
    /// The maximum number of tokens allowed in the model's response.
    pub max_tokens: u32,
}

/// The `LLMResponse` struct represents the response from a Language Model (LLM).
/// It includes the content generated by the model, the model name, and the usage statistics.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMResponse {
    /// The content generated by the model.
    pub content: String,
    /// The name of the model that generated the content.
    pub model: String,
    /// The usage statistics of the model for this request.
    pub usage: LLMUsage,
}

/// The `LLMUsage` struct represents the usage statistics of a Language Model (LLM).
/// It includes the number of tokens used for the prompt, the completion, and the total tokens.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMUsage {
    /// The number of tokens used for the prompt.
    pub prompt_tokens: u32,
    /// The number of tokens used for the completion.
    pub completion_tokens: u32,
    /// The total number of tokens used.
    pub total_tokens: u32,
}